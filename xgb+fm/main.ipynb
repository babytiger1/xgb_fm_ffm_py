{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xlearn as xl\n",
    "\n",
    "# ffm_model = xl.create_ffm() \n",
    "# ffm_model.setTrain(\"../dataset/all/train.csv\") \n",
    "# param = {'task':'binary', 'lr':0.2, 'lambda':0.002}\n",
    "\n",
    "# # Train model\n",
    "# ffm_model.fit(param, \"model.out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_path = \"../dataset/all/train.csv\"\n",
    "df = pd.read_csv(train_path,index_col=\"ID_code\")\n",
    "\n",
    "test_path = \"../dataset/all/test.csv\"\n",
    "df_eval = pd.read_csv(test_path,index_col=\"ID_code\")\n",
    "\n",
    "dfLabel = df['target']\n",
    "df = df.drop('target',axis=1)\n",
    "\n",
    "dfEvalLabel = df_eval\n",
    "\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "train_label = dfLabel[msk]\n",
    "test = df[~msk]\n",
    "test_label = dfLabel[~msk]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-93ce6c990130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtree_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/python27/lib/python2.7/site-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python27/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python27/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/python27/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1110\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#le = preprocessing.LabelEncoder()\n",
    "#dfLabel = le.fit(df['target']).transform(df['label'])\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "tree_model = XGBClassifier(\n",
    "silent=1 ,#设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "#nthread=4,# cpu 线程数 默认最大\n",
    "learning_rate= 0.01, # 如同学习率\n",
    "min_child_weight=1,\n",
    "nthread=4,  # 并行数\n",
    "# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "max_depth=5, # 构建树的深度，越大越容易过拟合\n",
    "gamma=0.1,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子，对叶节点个数的惩罚系数。\n",
    "subsample=1, # 随机采样训练样本 训练实例的子采样比\n",
    "max_delta_step=0,#最大增量步长，我们允许每个树的权重估计。\n",
    "colsample_bytree=1, # 生成树时进行的列采样\n",
    "reg_lambda=1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "#reg_alpha=0, # L1 正则项参数\n",
    "#scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n",
    "#objective= 'multi:softmax', #多分类的问题 指定学习任务和相应的学习目标\n",
    "#num_class=10, # 类别数，多分类与 multisoftmax 并用\n",
    "n_estimators=100, #树的个数\n",
    "seed=1000 #随机种子\n",
    "#eval_metric= 'auc'\n",
    ")\n",
    "\n",
    "tree_model.fit(train,train_label )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7108\n",
      "ACC: 0.9014\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "Precesion: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def eval_res(test_label,preds):\n",
    "\ty = test_label\n",
    "\tscores = preds\n",
    "\ty_pred = (scores >= 0.5)*1\n",
    "\tprint('AUC: %.4f' % metrics.roc_auc_score(y,scores))\n",
    "\tprint('ACC: %.4f' % metrics.accuracy_score(y,y_pred))\n",
    "\tprint('Recall: %.4f' % metrics.recall_score(y,y_pred))\n",
    "\tprint('F1-score: %.4f' %metrics.f1_score(y,y_pred))\n",
    "\tprint('Precesion: %.4f' %metrics.precision_score(y,y_pred))\n",
    "    \n",
    "preds = tree_model.predict_proba(test)   \n",
    "eval_res(test_label,preds[:,1])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(tree_model.n_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oh = tree_model.apply(train)\n",
    "tm_enc = OneHotEncoder(categories='auto',sparse = 'false')\n",
    "res_date = tm_enc.fit(train_oh).transform(train_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method csr_matrix.getcol of <160058x3169 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 16005800 stored elements in Compressed Sparse Row format>>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_date.getcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oh = tree_model.apply(test)\n",
    "test_date = tm_enc.transform(test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<160058x3169 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 16005800 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "###### 如果加入dense vector会强烈降低auc\n",
    "# train_add = np.hstack((train.values,res_date.toarray()))\n",
    "# test_add = np.hstack((test.values,test_date.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets \n",
    "sklearn.datasets.dump_svmlight_file(res_date.toarray() ,train_label ,\"aa.libsvm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.datasets.dump_svmlight_file(test_date.toarray() ,test_label ,\"bb.libsvm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlearn as xl\n",
    "\n",
    "ffm_model1 = xl.create_fm() \n",
    "ffm_model1.setTrain(\"aa.libsvm\") \n",
    "ffm_model1.setValidate(\"bb.libsvm\")\n",
    "\n",
    "ffm_model2 = xl.create_fm() \n",
    "ffm_model2.setTrain(\"aa.libsvm\") \n",
    "ffm_model2.setValidate(\"bb.libsvm\")\n",
    "\n",
    "ffm_model3 = xl.create_fm() \n",
    "ffm_model3.setTrain(\"aa.libsvm\") \n",
    "ffm_model3.setValidate(\"bb.libsvm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = {'task':'binary', 'lr':0.1, 'lambda':0.002,'epoch':20,'k':8,'opt':'sgd'}\n",
    "param2 = {'task':'binary', 'lr':0.1, 'lambda':0.002,'epoch':20,'k':8,'opt':'adagrad'}\n",
    "param3 = {'task':'binary', 'lr':0.1, 'lambda':0.002,'epoch':20,'k':8,'opt':'ftrl'}\n",
    "\n",
    "\n",
    "# Train model\n",
    "ffm_model1.fit(param1, \"model1.out\")\n",
    "ffm_model2.fit(param2, \"model2.out\")\n",
    "ffm_model3.fit(param3, \"model3.out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffm_model.setSign()\n",
    "# ffm_model.setTest(\"bb.libsvm\")\n",
    "# ffm_model.predict(\"model.out\", \"output_label.txt\")\n",
    "\n",
    "ffm_model1.setSigmoid()\n",
    "ffm_model1.setTest(\"bb.libsvm\")\n",
    "ffm_model1.predict(\"model1.out\", \"output1.txt\")\n",
    "\n",
    "ffm_model2.setSigmoid()\n",
    "ffm_model2.setTest(\"bb.libsvm\")\n",
    "ffm_model2.predict(\"model2.out\", \"output2.txt\")\n",
    "\n",
    "ffm_model3.setSigmoid()\n",
    "ffm_model3.setTest(\"bb.libsvm\")\n",
    "ffm_model3.predict(\"model3.out\", \"output3.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7467\n",
      "ACC: 0.9041\n",
      "Recall: 0.0851\n",
      "F1-score: 0.1489\n",
      "Precesion: 0.5961\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "preds = numpy.loadtxt('output1.txt')\n",
    "eval_res(test_label,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7474\n",
      "ACC: 0.9041\n",
      "Recall: 0.0782\n",
      "F1-score: 0.1386\n",
      "Precesion: 0.6063\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "preds = numpy.loadtxt('output2.txt')\n",
    "eval_res(test_label,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7521\n",
      "ACC: 0.9037\n",
      "Recall: 0.0879\n",
      "F1-score: 0.1525\n",
      "Precesion: 0.5748\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "preds = numpy.loadtxt('output.txt')\n",
    "eval_res(test_label,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
